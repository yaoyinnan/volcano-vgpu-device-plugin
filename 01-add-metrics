diff --git a/cmd/vgpu-monitor/build.sh b/cmd/vgpu-monitor/build.sh
deleted file mode 100644
index c6bfa72..0000000
--- a/cmd/vgpu-monitor/build.sh
+++ /dev/null
@@ -1,17 +0,0 @@
-#!/usr/bin/env bash
-# Copyright 2024 The HAMi Authors.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-protoc --go_out=. --go_opt=paths=source_relative --go-grpc_out=. --go-grpc_opt=paths=source_relative noderpc/noderpc.proto
-go build
diff --git a/cmd/vgpu-monitor/cudevshr.go b/cmd/vgpu-monitor/cudevshr.go
deleted file mode 100644
index 11d2f5b..0000000
--- a/cmd/vgpu-monitor/cudevshr.go
+++ /dev/null
@@ -1,103 +0,0 @@
-/*
-Copyright 2024 The HAMi Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"errors"
-	"fmt"
-	"os"
-	"syscall"
-	"unsafe"
-
-	"golang.org/x/exp/mmap"
-)
-
-type deviceMemory struct {
-	contextSize uint64
-	moduleSize  uint64
-	bufferSize  uint64
-	offset      uint64
-	total       uint64
-}
-
-type shrregProcSlotT struct {
-	pid         int32
-	hostpid     int32
-	used        [16]deviceMemory
-	monitorused [16]uint64
-	status      int32
-}
-
-type uuid struct {
-	uuid [96]byte
-}
-
-type semT struct {
-	sem [32]byte
-}
-
-type sharedRegionT struct {
-	initializedFlag int32
-	smInitFlag      int32
-	ownerPid        uint32
-	sem             semT
-	num             uint64
-	uuids           [16]uuid
-
-	limit   [16]uint64
-	smLimit [16]uint64
-	procs   [1024]shrregProcSlotT
-
-	procnum           int32
-	utilizationSwitch int32
-	recentKernel      int32
-	priority          int32
-}
-
-type nvidiaCollector struct {
-	// Exposed for testing
-	cudevshrPath string
-	at           *mmap.ReaderAt
-	cudaCache    *sharedRegionT
-}
-
-func mmapcachefile(filename string, nc *nvidiaCollector) error {
-	var m = &sharedRegionT{}
-	f, err := os.OpenFile(filename, os.O_RDWR, 0666)
-	if err != nil {
-		fmt.Println("openfile error=", err.Error())
-		return err
-	}
-	data, err := syscall.Mmap(int(f.Fd()), 0, int(unsafe.Sizeof(*m)), syscall.PROT_WRITE|syscall.PROT_READ, syscall.MAP_SHARED)
-	if err != nil {
-		return err
-	}
-	var cachestr *sharedRegionT = *(**sharedRegionT)(unsafe.Pointer(&data))
-	fmt.Println("sizeof=", unsafe.Sizeof(*m), "cachestr=", cachestr.utilizationSwitch, cachestr.recentKernel)
-	nc.cudaCache = cachestr
-	return nil
-}
-
-func getvGPUMemoryInfo(nc *nvidiaCollector) (*sharedRegionT, error) {
-	if len(nc.cudevshrPath) > 0 {
-		if nc.cudaCache == nil {
-			mmapcachefile(nc.cudevshrPath, nc)
-		}
-		return nc.cudaCache, nil
-	}
-	return &sharedRegionT{}, errors.New("not found path")
-}
diff --git a/cmd/vgpu-monitor/feedback.go b/cmd/vgpu-monitor/feedback.go
index 9953c50..4cf77f7 100644
--- a/cmd/vgpu-monitor/feedback.go
+++ b/cmd/vgpu-monitor/feedback.go
@@ -19,24 +19,21 @@ package main
 import (
 	"time"
 
+	"volcano.sh/k8s-device-plugin/pkg/monitor/nvidia"
+
 	"github.com/NVIDIA/go-nvml/pkg/nvml"
 	"k8s.io/klog/v2"
 )
 
 type UtilizationPerDevice []int
 
-var srPodList map[string]podusage
-
-func init() {
-	srPodList = make(map[string]podusage)
-}
-
-func CheckBlocking(utSwitchOn map[string]UtilizationPerDevice, p int, pu podusage) bool {
-	for _, devuuid := range pu.sr.uuids {
-		_, ok := utSwitchOn[string(devuuid.uuid[:])]
+func CheckBlocking(utSwitchOn map[string]UtilizationPerDevice, p int, c *nvidia.ContainerUsage) bool {
+	for i := 0; i < c.Info.DeviceMax(); i++ {
+		uuid := c.Info.DeviceUUID(i)
+		_, ok := utSwitchOn[uuid]
 		if ok {
 			for i := 0; i < p; i++ {
-				if utSwitchOn[string(devuuid.uuid[:])][i] > 0 {
+				if utSwitchOn[uuid][i] > 0 {
 					return true
 				}
 			}
@@ -47,16 +44,17 @@ func CheckBlocking(utSwitchOn map[string]UtilizationPerDevice, p int, pu podusag
 }
 
 // Check whether task with higher priority use GPU or there are other tasks with the same priority.
-func CheckPriority(utSwitchOn map[string]UtilizationPerDevice, p int, pu podusage) bool {
-	for _, devuuid := range pu.sr.uuids {
-		_, ok := utSwitchOn[string(devuuid.uuid[:])]
+func CheckPriority(utSwitchOn map[string]UtilizationPerDevice, p int, c *nvidia.ContainerUsage) bool {
+	for i := 0; i < c.Info.DeviceMax(); i++ {
+		uuid := c.Info.DeviceUUID(i)
+		_, ok := utSwitchOn[uuid]
 		if ok {
 			for i := 0; i < p; i++ {
-				if utSwitchOn[string(devuuid.uuid[:])][i] > 0 {
+				if utSwitchOn[uuid][i] > 0 {
 					return true
 				}
 			}
-			if utSwitchOn[string(devuuid.uuid[:])][p] > 1 {
+			if utSwitchOn[uuid][p] > 1 {
 				return true
 			}
 		}
@@ -64,72 +62,72 @@ func CheckPriority(utSwitchOn map[string]UtilizationPerDevice, p int, pu podusag
 	return false
 }
 
-func Observe(srlist *map[string]podusage) error {
+func Observe(lister *nvidia.ContainerLister) {
 	utSwitchOn := map[string]UtilizationPerDevice{}
+	containers := lister.ListContainers()
 
-	for idx, val := range *srlist {
-		if val.sr == nil {
-			continue
-		}
-		if val.sr.recentKernel > 0 {
-			(*srlist)[idx].sr.recentKernel--
-			if (*srlist)[idx].sr.recentKernel > 0 {
-				for _, devuuid := range val.sr.uuids {
+	for _, c := range containers {
+		recentKernel := c.Info.GetRecentKernel()
+		if recentKernel > 0 {
+			recentKernel--
+			if recentKernel > 0 {
+				for i := 0; i < c.Info.DeviceMax(); i++ {
 					// Null device condition
-					if devuuid.uuid[0] == 0 {
+					if !c.Info.IsValidUUID(i) {
 						continue
 					}
-					if len(utSwitchOn[string(devuuid.uuid[:])]) == 0 {
-						utSwitchOn[string(devuuid.uuid[:])] = []int{0, 0}
+					uuid := c.Info.DeviceUUID(i)
+					if len(utSwitchOn[uuid]) == 0 {
+						utSwitchOn[uuid] = []int{0, 0}
 					}
-					utSwitchOn[string(devuuid.uuid[:])][val.sr.priority]++
+					utSwitchOn[uuid][c.Info.GetPriority()]++
 				}
 			}
+			c.Info.SetRecentKernel(recentKernel)
 		}
 	}
-	for idx, val := range *srlist {
-		if val.sr == nil {
-			continue
-		}
-		if CheckBlocking(utSwitchOn, int(val.sr.priority), val) {
-			if (*srlist)[idx].sr.recentKernel >= 0 {
+	for idx, c := range containers {
+		priority := c.Info.GetPriority()
+		recentKernel := c.Info.GetRecentKernel()
+		utilizationSwitch := c.Info.GetUtilizationSwitch()
+		if CheckBlocking(utSwitchOn, priority, c) {
+			if recentKernel >= 0 {
 				klog.Infof("utSwitchon=%v", utSwitchOn)
 				klog.Infof("Setting Blocking to on %v", idx)
-				(*srlist)[idx].sr.recentKernel = -1
+				c.Info.SetRecentKernel(-1)
 			}
 		} else {
-			if (*srlist)[idx].sr.recentKernel < 0 {
+			if recentKernel < 0 {
 				klog.Infof("utSwitchon=%v", utSwitchOn)
 				klog.Infof("Setting Blocking to off %v", idx)
-				(*srlist)[idx].sr.recentKernel = 0
+				c.Info.SetRecentKernel(0)
 			}
 		}
-		if CheckPriority(utSwitchOn, int(val.sr.priority), val) {
-			if (*srlist)[idx].sr.utilizationSwitch != 1 {
+		if CheckPriority(utSwitchOn, priority, c) {
+			if utilizationSwitch != 1 {
 				klog.Infof("utSwitchon=%v", utSwitchOn)
 				klog.Infof("Setting UtilizationSwitch to on %v", idx)
-				(*srlist)[idx].sr.utilizationSwitch = 1
+				c.Info.SetUtilizationSwitch(1)
 			}
 		} else {
-			if (*srlist)[idx].sr.utilizationSwitch != 0 {
+			if utilizationSwitch != 0 {
 				klog.Infof("utSwitchon=%v", utSwitchOn)
 				klog.Infof("Setting UtilizationSwitch to off %v", idx)
-				(*srlist)[idx].sr.utilizationSwitch = 0
+				c.Info.SetUtilizationSwitch(0)
 			}
 		}
 	}
-	return nil
 }
 
-func watchAndFeedback() {
+func watchAndFeedback(lister *nvidia.ContainerLister) {
 	nvml.Init()
 	for {
 		time.Sleep(time.Second * 5)
-		err := monitorPath(srPodList)
+		err := lister.Update()
 		if err != nil {
-			klog.Errorf("monitorPath failed %v", err.Error())
+			klog.Errorf("Failed to update container list: %v", err)
+			continue
 		}
-		klog.Infof("WatchAndFeedback srPodList=%v", srPodList)
-		Observe(&srPodList)
+		Observe(lister)
 	}
 }
diff --git a/cmd/vgpu-monitor/main.go b/cmd/vgpu-monitor/main.go
index 14ff97b..8f18972 100644
--- a/cmd/vgpu-monitor/main.go
+++ b/cmd/vgpu-monitor/main.go
@@ -17,6 +17,8 @@ limitations under the License.
 package main
 
 import (
+	"volcano.sh/k8s-device-plugin/pkg/monitor/nvidia"
+
 	"k8s.io/klog/v2"
 )
 
@@ -24,9 +26,13 @@ func main() {
 	if err := ValidateEnvVars(); err != nil {
 		klog.Fatalf("Failed to validate environment variables: %v", err)
 	}
+	containerLister, err := nvidia.NewContainerLister()
+	if err != nil {
+		klog.Fatalf("Failed to create container lister: %v", err)
+	}
 	errchannel := make(chan error)
-	go initMetrics()
-	go watchAndFeedback()
+	go initMetrics(containerLister)
+	go watchAndFeedback(containerLister)
 	for {
 		err := <-errchannel
 		klog.Errorf("failed to serve: %v", err)
diff --git a/cmd/vgpu-monitor/metrics.go b/cmd/vgpu-monitor/metrics.go
index aa2d96e..db53c6f 100644
--- a/cmd/vgpu-monitor/metrics.go
+++ b/cmd/vgpu-monitor/metrics.go
@@ -23,15 +23,15 @@ import (
 	"strings"
 	"time"
 
+	"volcano.sh/k8s-device-plugin/pkg/monitor/nvidia"
+
 	"github.com/NVIDIA/go-nvml/pkg/nvml"
 	"github.com/prometheus/client_golang/prometheus"
 	"github.com/prometheus/client_golang/prometheus/promhttp"
 
 	"k8s.io/apimachinery/pkg/labels"
 	"k8s.io/client-go/informers"
-	"k8s.io/client-go/kubernetes"
 	listerscorev1 "k8s.io/client-go/listers/core/v1"
-	"k8s.io/client-go/rest"
 	"k8s.io/klog/v2"
 )
 
@@ -49,7 +49,8 @@ import (
 type ClusterManager struct {
 	Zone string
 	// Contains many more fields not listed in this example.
-	PodLister listerscorev1.PodLister
+	PodLister       listerscorev1.PodLister
+	containerLister *nvidia.ContainerLister
 }
 
 // ReallyExpensiveAssessmentOfTheSystemState is a mock for the data gathering a
@@ -106,7 +107,16 @@ var (
 		"Container device meory description",
 		[]string{"podnamespace", "podname", "ctrname", "vdeviceid", "deviceuuid", "context", "module", "data", "offset"}, nil,
 	)
-	clientset *kubernetes.Clientset
+	ctrDeviceUtilizationdesc = prometheus.NewDesc(
+		"Device_utilization_desc_of_container",
+		"Container device utilization description",
+		[]string{"podnamespace", "podname", "ctrname", "vdeviceid", "deviceuuid"}, nil,
+	)
+	ctrDeviceLastKernelDesc = prometheus.NewDesc(
+		"Device_last_kernel_of_container",
+		"Container device last kernel description",
+		[]string{"podnamespace", "podname", "ctrname", "vdeviceid", "deviceuuid"}, nil,
+	)
 )
 
 // Describe is implemented with DescribeByCollect. That's possible because the
@@ -120,137 +130,153 @@ func (cc ClusterManagerCollector) Describe(ch chan<- *prometheus.Desc) {
 	//prometheus.DescribeByCollect(cc, ch)
 }
 
-func gettotalusage(usage podusage, vidx int) (deviceMemory, error) {
-	added := deviceMemory{
-		bufferSize:  0,
-		contextSize: 0,
-		moduleSize:  0,
-		offset:      0,
-		total:       0,
-	}
-	for _, val := range usage.sr.procs {
-		added.bufferSize += val.used[vidx].bufferSize
-		added.contextSize += val.used[vidx].contextSize
-		added.moduleSize += val.used[vidx].moduleSize
-		added.offset += val.used[vidx].offset
-		added.total += val.used[vidx].total
-	}
-	return added, nil
-}
-
 // Collect first triggers the ReallyExpensiveAssessmentOfTheSystemState. Then it
 // creates constant metrics for each host on the fly based on the returned data.
 //
 // Note that Collect could be called concurrently, so we depend on
 // ReallyExpensiveAssessmentOfTheSystemState to be concurrency-safe.
 func (cc ClusterManagerCollector) Collect(ch chan<- prometheus.Metric) {
-	klog.Info("Starting to collect metrics for volcano vgpu monitor")
-	if srPodList == nil {
-		srPodList = make(map[string]podusage)
+	klog.Info("Starting to collect metrics for vGPUMonitor")
+	containerLister := cc.ClusterManager.containerLister
+	if err := containerLister.Update(); err != nil {
+		klog.Error("Update container error: %s", err.Error())
 	}
-	if err := monitorPath(srPodList); err != nil {
-		klog.Error("err=", err.Error())
+
+	nvret := nvml.Init()
+	if nvret != nvml.SUCCESS {
+		klog.Error("nvml Init err=", nvml.ErrorString(nvret))
 	}
-	if clientset != nil {
-		nvret := nvml.Init()
-		if nvret != nvml.SUCCESS {
-			klog.Error("nvml Init err=", nvml.ErrorString(nvret))
+	devnum, nvret := nvml.DeviceGetCount()
+	if nvret != nvml.SUCCESS {
+		klog.Error("nvml GetDeviceCount err=", nvml.ErrorString(nvret))
+	} else {
+		for ii := 0; ii < devnum; ii++ {
+			hdev, nvret := nvml.DeviceGetHandleByIndex(ii)
+			if nvret != nvml.SUCCESS {
+				klog.Error(nvml.ErrorString(nvret))
+			}
+			memoryUsed := 0
+			memory, ret := hdev.GetMemoryInfo()
+			if ret == nvml.SUCCESS {
+				memoryUsed = int(memory.Used)
+			} else {
+				klog.Error("nvml get memory error ret=", ret)
+			}
+
+			uuid, nvret := hdev.GetUUID()
+			if nvret != nvml.SUCCESS {
+				klog.Error(nvml.ErrorString(nvret))
+			} else {
+				ch <- prometheus.MustNewConstMetric(
+					hostGPUdesc,
+					prometheus.GaugeValue,
+					float64(memoryUsed),
+					fmt.Sprint(ii), uuid,
+				)
+			}
+			util, nvret := hdev.GetUtilizationRates()
+			if nvret != nvml.SUCCESS {
+				klog.Error(nvml.ErrorString(nvret))
+			} else {
+				ch <- prometheus.MustNewConstMetric(
+					hostGPUUtilizationdesc,
+					prometheus.GaugeValue,
+					float64(util.Gpu),
+					fmt.Sprint(ii), uuid,
+				)
+			}
+
 		}
-		devnum, nvret := nvml.DeviceGetCount()
-		if nvret != nvml.SUCCESS {
-			klog.Error("nvml GetDeviceCount err=", nvml.ErrorString(nvret))
-		} else {
-			for ii := 0; ii < devnum; ii++ {
-				hdev, nvret := nvml.DeviceGetHandleByIndex(ii)
-				if nvret != nvml.SUCCESS {
-					klog.Error(nvml.ErrorString(nvret))
+	}
+
+	pods, err := cc.ClusterManager.PodLister.List(labels.Everything())
+	if err != nil {
+		klog.Error("failed to list pods with err=", err.Error())
+	}
+	nowSec := time.Now().Unix()
+
+	containers := containerLister.ListContainers()
+	for _, pod := range pods {
+		for _, c := range containers {
+			//for sridx := range srPodList {
+			//	if srPodList[sridx].sr == nil {
+			//		continue
+			//	}
+			if c.Info == nil {
+				continue
+			}
+			//podUID := strings.Split(srPodList[sridx].idstr, "_")[0]
+			//ctrName := strings.Split(srPodList[sridx].idstr, "_")[1]
+			podUID := c.PodUID
+			ctrName := c.ContainerName
+			if strings.Compare(string(pod.UID), podUID) != 0 {
+				continue
+			}
+			fmt.Println("Pod matched!", pod.Name, pod.Namespace, pod.Labels)
+			for _, ctr := range pod.Spec.Containers {
+				if strings.Compare(ctr.Name, ctrName) != 0 {
+					continue
 				}
-				memoryUsed := 0
-				memory, ret := hdev.GetMemoryInfo()
-				if ret == nvml.SUCCESS {
-					memoryUsed = int(memory.Used)
-				} else {
-					klog.Error("nvml get memory error ret=", ret)
+				fmt.Println("container matched", ctr.Name)
+				//err := setHostPid(pod, pod.Status.ContainerStatuses[ctridx], &srPodList[sridx])
+				//if err != nil {
+				//	fmt.Println("setHostPid filed", err.Error())
+				//}
+				//fmt.Println("sr.list=", srPodList[sridx].sr)
+				podlabels := make(map[string]string)
+				for idx, val := range pod.Labels {
+					idxfix := strings.ReplaceAll(idx, "-", "_")
+					valfix := strings.ReplaceAll(val, "-", "_")
+					podlabels[idxfix] = valfix
 				}
+				for i := 0; i < c.Info.DeviceNum(); i++ {
+					uuid := c.Info.DeviceUUID(i)[0:40]
+					memoryTotal := c.Info.DeviceMemoryTotal(i)
+					memoryLimit := c.Info.DeviceMemoryLimit(i)
+					memoryContextSize := c.Info.DeviceMemoryContextSize(i)
+					memoryModuleSize := c.Info.DeviceMemoryModuleSize(i)
+					memoryBufferSize := c.Info.DeviceMemoryBufferSize(i)
+					memoryOffset := c.Info.DeviceMemoryOffset(i)
+					smUtil := c.Info.DeviceSmUtil(i)
+					lastKernelTime := c.Info.LastKernelTime()
 
-				uuid, nvret := hdev.GetUUID()
-				if nvret != nvml.SUCCESS {
-					klog.Error(nvml.ErrorString(nvret))
-				} else {
+					//fmt.Println("uuid=", uuid, "length=", len(uuid))
 					ch <- prometheus.MustNewConstMetric(
-						hostGPUdesc,
+						ctrvGPUdesc,
 						prometheus.GaugeValue,
-						float64(memoryUsed),
-						fmt.Sprint(ii), uuid,
+						float64(memoryTotal),
+						pod.Namespace, pod.Name, ctrName, fmt.Sprint(i), uuid, /*,string(sr.sr.uuids[i].uuid[:])*/
 					)
-				}
-				util, nvret := hdev.GetUtilizationRates()
-				if nvret != nvml.SUCCESS {
-					klog.Error(nvml.ErrorString(nvret))
-				} else {
 					ch <- prometheus.MustNewConstMetric(
-						hostGPUUtilizationdesc,
+						ctrvGPUlimitdesc,
 						prometheus.GaugeValue,
-						float64(util.Gpu),
-						fmt.Sprint(ii), uuid,
+						float64(memoryLimit),
+						pod.Namespace, pod.Name, ctrName, fmt.Sprint(i), uuid, /*,string(sr.sr.uuids[i].uuid[:])*/
 					)
-				}
-
-			}
-		}
-
-		pods, err := cc.ClusterManager.PodLister.List(labels.Everything())
-		if err != nil {
-			klog.Error("failed to list pods with err=", err.Error())
-		}
-		for _, val := range pods {
-			for sridx := range srPodList {
-				if srPodList[sridx].sr == nil {
-					continue
-				}
-				podUID := strings.Split(srPodList[sridx].idstr, "_")[0]
-				ctrName := strings.Split(srPodList[sridx].idstr, "_")[1]
-				if strings.Compare(string(val.UID), podUID) == 0 {
-					fmt.Println("Pod matched!", val.Name, val.Namespace, val.Labels)
-					for _, ctr := range val.Spec.Containers {
-						if strings.Compare(ctr.Name, ctrName) == 0 {
-							fmt.Println("container matched", ctr.Name)
-							//err := setHostPid(val, val.Status.ContainerStatuses[ctridx], &srPodList[sridx])
-							//if err != nil {
-							//	fmt.Println("setHostPid filed", err.Error())
-							//}
-							//fmt.Println("sr.list=", srPodList[sridx].sr)
-							podlabels := make(map[string]string)
-							for idx, val := range val.Labels {
-								idxfix := strings.ReplaceAll(idx, "-", "_")
-								valfix := strings.ReplaceAll(val, "-", "_")
-								podlabels[idxfix] = valfix
-							}
-							for i := 0; i < int(srPodList[sridx].sr.num); i++ {
-								value, _ := gettotalusage(srPodList[sridx], i)
-								uuid := string(srPodList[sridx].sr.uuids[i].uuid[:])[0:40]
-
-								//fmt.Println("uuid=", uuid, "length=", len(uuid))
-								ch <- prometheus.MustNewConstMetric(
-									ctrvGPUdesc,
-									prometheus.GaugeValue,
-									float64(value.total),
-									val.Namespace, val.Name, ctrName, fmt.Sprint(i), uuid, /*,string(sr.sr.uuids[i].uuid[:])*/
-								)
-								ch <- prometheus.MustNewConstMetric(
-									ctrvGPUlimitdesc,
-									prometheus.GaugeValue,
-									float64(srPodList[sridx].sr.limit[i]),
-									val.Namespace, val.Name, ctrName, fmt.Sprint(i), uuid, /*,string(sr.sr.uuids[i].uuid[:])*/
-								)
-								ch <- prometheus.MustNewConstMetric(
-									ctrDeviceMemorydesc,
-									prometheus.CounterValue,
-									float64(value.total),
-									val.Namespace, val.Name, ctrName, fmt.Sprint(i), uuid, fmt.Sprint(value.contextSize), fmt.Sprint(value.moduleSize), fmt.Sprint(value.bufferSize), fmt.Sprint(value.offset),
-								)
-							}
+					ch <- prometheus.MustNewConstMetric(
+						ctrDeviceMemorydesc,
+						prometheus.CounterValue,
+						float64(memoryTotal),
+						pod.Namespace, pod.Name, ctrName, fmt.Sprint(i), uuid,
+						fmt.Sprint(memoryContextSize), fmt.Sprint(memoryModuleSize), fmt.Sprint(memoryBufferSize), fmt.Sprint(memoryOffset),
+					)
+					ch <- prometheus.MustNewConstMetric(
+						ctrDeviceUtilizationdesc,
+						prometheus.GaugeValue,
+						float64(smUtil),
+						pod.Namespace, pod.Name, ctrName, fmt.Sprint(i), uuid,
+					)
+					if lastKernelTime > 0 {
+						lastSec := nowSec - lastKernelTime
+						if lastSec < 0 {
+							lastSec = 0
 						}
+						ch <- prometheus.MustNewConstMetric(
+							ctrDeviceLastKernelDesc,
+							prometheus.GaugeValue,
+							float64(lastSec),
+							pod.Namespace, pod.Name, ctrName, fmt.Sprint(i), uuid,
+						)
 					}
 				}
 			}
@@ -263,12 +289,13 @@ func (cc ClusterManagerCollector) Collect(ch chan<- prometheus.Metric) {
 // ClusterManager. Finally, it registers the ClusterManagerCollector with a
 // wrapping Registerer that adds the zone as a label. In this way, the metrics
 // collected by different ClusterManagerCollectors do not collide.
-func NewClusterManager(zone string, reg prometheus.Registerer) *ClusterManager {
+func NewClusterManager(zone string, reg prometheus.Registerer, containerLister *nvidia.ContainerLister) *ClusterManager {
 	c := &ClusterManager{
-		Zone: zone,
+		Zone:            zone,
+		containerLister: containerLister,
 	}
 
-	informerFactory := informers.NewSharedInformerFactoryWithOptions(clientset, time.Hour*1)
+	informerFactory := informers.NewSharedInformerFactoryWithOptions(containerLister.Clientset(), time.Hour*1)
 	c.PodLister = informerFactory.Core().V1().Pods().Lister()
 	stopCh := make(chan struct{})
 	informerFactory.Start(stopCh)
@@ -278,26 +305,16 @@ func NewClusterManager(zone string, reg prometheus.Registerer) *ClusterManager {
 	return c
 }
 
-func initMetrics() {
+func initMetrics(containerLister *nvidia.ContainerLister) {
 	// Since we are dealing with custom Collector implementations, it might
 	// be a good idea to try it out with a pedantic registry.
-	klog.Info("Initializing metrics for volcano vgpu monitor")
+	klog.Info("Initializing metrics for vGPUmonitor")
 	reg := prometheus.NewRegistry()
 	//reg := prometheus.NewPedanticRegistry()
-	config, err := rest.InClusterConfig()
-	if err != nil {
-		fmt.Println(err.Error())
-		return
-	}
-	clientset, err = kubernetes.NewForConfig(config)
-	if err != nil {
-		fmt.Println(err.Error())
-		return
-	}
 
 	// Construct cluster managers. In real code, we would assign them to
 	// variables to then do something with them.
-	NewClusterManager("vGPU", reg)
+	NewClusterManager("vGPU", reg, containerLister)
 
 	http.Handle("/metrics", promhttp.HandlerFor(reg, promhttp.HandlerOpts{}))
 	log.Fatal(http.ListenAndServe(":9394", nil))
diff --git a/cmd/vgpu-monitor/pathmonitor.go b/cmd/vgpu-monitor/pathmonitor.go
deleted file mode 100644
index d3a7b6b..0000000
--- a/cmd/vgpu-monitor/pathmonitor.go
+++ /dev/null
@@ -1,151 +0,0 @@
-/*
-Copyright 2024 The HAMi Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"context"
-	"errors"
-	"fmt"
-	"os"
-	"path/filepath"
-	"strings"
-	"sync"
-	"time"
-
-	corev1 "k8s.io/api/core/v1"
-	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
-	"k8s.io/klog/v2"
-)
-
-type podusage struct {
-	idstr string
-	sr    *sharedRegionT
-}
-
-var (
-	containerPath string
-	nodeName      string
-	lock          sync.Mutex
-)
-
-func init() {
-	hookPath, ok := os.LookupEnv("HOOK_PATH")
-	if ok {
-		containerPath = filepath.Join(hookPath, "containers")
-	}
-	nodeName = os.Getenv("NODE_NAME")
-}
-
-func checkfiles(fpath string) (*sharedRegionT, error) {
-	klog.Infof("Checking path %s", fpath)
-	files, err := os.ReadDir(fpath)
-	if err != nil {
-		return nil, err
-	}
-	if len(files) > 2 {
-		return nil, errors.New("cache num not matched")
-	}
-	if len(files) == 0 {
-		return nil, nil
-	}
-	for _, val := range files {
-		if strings.Contains(val.Name(), "libvgpu.so") {
-			continue
-		}
-		if !strings.Contains(val.Name(), ".cache") {
-			continue
-		}
-		cachefile := fpath + "/" + val.Name()
-		nc := nvidiaCollector{
-			cudevshrPath: cachefile,
-			at:           nil,
-		}
-		sr, err := getvGPUMemoryInfo(&nc)
-		if err != nil {
-			klog.Errorf("getvGPUMemoryInfo failed: %v", err)
-		} else {
-			klog.Infof("getvGPUMemoryInfo success with utilizationSwitch=%d, recentKernel=%d, priority=%d", sr.utilizationSwitch, sr.recentKernel, sr.priority)
-			return sr, nil
-		}
-	}
-	return nil, nil
-}
-
-func isVaildPod(name string, pods *corev1.PodList) bool {
-	for _, val := range pods.Items {
-		if strings.Contains(name, string(val.UID)) {
-			return true
-		}
-	}
-	return false
-}
-
-func monitorPath(podmap map[string]podusage) error {
-	lock.Lock()
-	defer lock.Unlock()
-	files, err := os.ReadDir(containerPath)
-	if err != nil {
-		return err
-	}
-	pods, err := clientset.CoreV1().Pods("").List(context.Background(), metav1.ListOptions{
-		FieldSelector: fmt.Sprintf("spec.nodeName=%s", nodeName),
-	})
-	if err != nil {
-		klog.Errorf("Failed to get pods on node %s, error: %v", nodeName, err)
-		return nil
-	}
-	klog.Infof("Found %d pods on node %s", len(pods.Items), nodeName)
-
-	for _, containerFile := range files {
-		dirname := containerPath + "/" + containerFile.Name()
-		if info, err1 := os.Stat(dirname); err1 != nil || !isVaildPod(info.Name(), pods) {
-			if info.ModTime().Add(time.Second * 300).Before(time.Now()) {
-				klog.Infof("Removing dirname %s in in monitorPath", dirname)
-				//syscall.Munmap(unsafe.Pointer(podmap[dirname].sr))
-				delete(podmap, dirname)
-				err2 := os.RemoveAll(dirname)
-				if err2 != nil {
-					klog.Errorf("Failed to remove dirname: %s , error: %v", dirname, err)
-					return err2
-				}
-			}
-		} else {
-			_, ok := podmap[dirname]
-			if !ok {
-				klog.Infof("Adding ctr dirname %s in monitorPath", dirname)
-				sharedRegion, err2 := checkfiles(dirname)
-				if err2 != nil {
-					klog.Errorf("Failed to checkfiles dirname: %s , error: %v", dirname, err)
-					return err2
-				}
-				if sharedRegion == nil {
-					klog.Infof("nil shared region for dirname %s in monitorPath", dirname)
-					continue
-				}
-
-				klog.Infof("Shared region after checking files: %v", *sharedRegion)
-				podmap[dirname] = podusage{
-					idstr: containerFile.Name(),
-					sr:    sharedRegion,
-				}
-			}
-		}
-	}
-
-	klog.Infof("Monitored path map: %v", podmap)
-	return nil
-}
diff --git a/cmd/vgpu-monitor/pathmonitor_test.go b/cmd/vgpu-monitor/pathmonitor_test.go
deleted file mode 100644
index 0f156da..0000000
--- a/cmd/vgpu-monitor/pathmonitor_test.go
+++ /dev/null
@@ -1,61 +0,0 @@
-/*
-Copyright 2024 The HAMi Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"testing"
-
-	corev1 "k8s.io/api/core/v1"
-	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
-)
-
-func TestIsVaildPod(t *testing.T) {
-	pods := &corev1.PodList{
-		Items: []corev1.Pod{
-			{
-				ObjectMeta: metav1.ObjectMeta{
-					UID: "123",
-				},
-			},
-			{
-				ObjectMeta: metav1.ObjectMeta{
-					UID: "456",
-				},
-			},
-		},
-	}
-
-	cases := []struct {
-		name     string
-		expected bool
-	}{
-		{
-			name:     "123",
-			expected: true,
-		},
-		{
-			name:     "789",
-			expected: false,
-		},
-	}
-
-	for _, c := range cases {
-		if got := isVaildPod(c.name, pods); got != c.expected {
-			t.Errorf("isVaildPod(%q) == %v, want %v", c.name, got, c.expected)
-		}
-	}
-}
diff --git a/cmd/vgpu-monitor/testcollector/main.go b/cmd/vgpu-monitor/testcollector/main.go
deleted file mode 100644
index bf13427..0000000
--- a/cmd/vgpu-monitor/testcollector/main.go
+++ /dev/null
@@ -1,145 +0,0 @@
-/*
-Copyright 2024 The HAMi Authors.
-
-Licensed under the Apache License, Version 2.0 (the "License");
-you may not use this file except in compliance with the License.
-You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-Unless required by applicable law or agreed to in writing, software
-distributed under the License is distributed on an "AS IS" BASIS,
-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-See the License for the specific language governing permissions and
-limitations under the License.
-*/
-
-package main
-
-import (
-	"log"
-	"net/http"
-
-	"github.com/prometheus/client_golang/prometheus"
-	"github.com/prometheus/client_golang/prometheus/promhttp"
-)
-
-// ClusterManager is an example for a system that might have been built without
-// Prometheus in mind. It models a central manager of jobs running in a
-// cluster. Thus, we implement a custom Collector called
-// ClusterManagerCollector, which collects information from a ClusterManager
-// using its provided methods and turns them into Prometheus Metrics for
-// collection.
-//
-// An additional challenge is that multiple instances of the ClusterManager are
-// run within the same binary, each in charge of a different zone. We need to
-// make use of wrapping Registerers to be able to register each
-// ClusterManagerCollector instance with Prometheus.
-type ClusterManager struct {
-	Zone string
-	// Contains many more fields not listed in this example.
-}
-
-// ReallyExpensiveAssessmentOfTheSystemState is a mock for the data gathering a
-// real cluster manager would have to do. Since it may actually be really
-// expensive, it must only be called once per collection. This implementation,
-// obviously, only returns some made-up data.
-func (c *ClusterManager) ReallyExpensiveAssessmentOfTheSystemState() (
-	oomCountByHost map[string]int, ramUsageByHost map[string]float64,
-) {
-	// Just example fake data.
-	oomCountByHost = map[string]int{
-		"foo.example.org": 42,
-		"bar.example.org": 2001,
-	}
-	ramUsageByHost = map[string]float64{
-		"foo.example.org": 6.023e23,
-		"bar.example.org": 3.14,
-	}
-	return
-}
-
-// ClusterManagerCollector implements the Collector interface.
-type ClusterManagerCollector struct {
-	ClusterManager *ClusterManager
-}
-
-// Descriptors used by the ClusterManagerCollector below.
-var (
-	oomCountDesc = prometheus.NewDesc(
-		"clustermanager_oom_crashes_total",
-		"Number of OOM crashes.",
-		[]string{"host"}, nil,
-	)
-	ramUsageDesc = prometheus.NewDesc(
-		"clustermanager_ram_usage_bytes",
-		"RAM usage as reported to the cluster manager.",
-		[]string{"host"}, nil,
-	)
-)
-
-// Describe is implemented with DescribeByCollect. That's possible because the
-// Collect method will always return the same two metrics with the same two
-// descriptors.
-func (cc ClusterManagerCollector) Describe(ch chan<- *prometheus.Desc) {
-	prometheus.DescribeByCollect(cc, ch)
-}
-
-// Collect first triggers the ReallyExpensiveAssessmentOfTheSystemState. Then it
-// creates constant metrics for each host on the fly based on the returned data.
-//
-// Note that Collect could be called concurrently, so we depend on
-// ReallyExpensiveAssessmentOfTheSystemState to be concurrency-safe.
-func (cc ClusterManagerCollector) Collect(ch chan<- prometheus.Metric) {
-	oomCountByHost, ramUsageByHost := cc.ClusterManager.ReallyExpensiveAssessmentOfTheSystemState()
-	for host, oomCount := range oomCountByHost {
-		ch <- prometheus.MustNewConstMetric(
-			oomCountDesc,
-			prometheus.CounterValue,
-			float64(oomCount),
-			host,
-		)
-	}
-	for host, ramUsage := range ramUsageByHost {
-		ch <- prometheus.MustNewConstMetric(
-			ramUsageDesc,
-			prometheus.GaugeValue,
-			ramUsage,
-			host,
-		)
-	}
-}
-
-// NewClusterManager first creates a Prometheus-ignorant ClusterManager
-// instance. Then, it creates a ClusterManagerCollector for the just created
-// ClusterManager. Finally, it registers the ClusterManagerCollector with a
-// wrapping Registerer that adds the zone as a label. In this way, the metrics
-// collected by different ClusterManagerCollectors do not collide.
-func NewClusterManager(zone string, reg prometheus.Registerer) *ClusterManager {
-	c := &ClusterManager{
-		Zone: zone,
-	}
-	cc := ClusterManagerCollector{ClusterManager: c}
-	prometheus.WrapRegistererWith(prometheus.Labels{"zone": zone}, reg).MustRegister(cc)
-	return c
-}
-
-func main() {
-	// Since we are dealing with custom Collector implementations, it might
-	// be a good idea to try it out with a pedantic registry.
-	reg := prometheus.NewPedanticRegistry()
-
-	// Construct cluster managers. In real code, we would assign them to
-	// variables to then do something with them.
-	NewClusterManager("db", reg)
-	NewClusterManager("ca", reg)
-
-	// Add the standard process and Go metrics to the custom registry.
-	reg.MustRegister(
-		prometheus.NewProcessCollector(prometheus.ProcessCollectorOpts{}),
-		prometheus.NewGoCollector(),
-	)
-
-	http.Handle("/metrics", promhttp.HandlerFor(reg, promhttp.HandlerOpts{}))
-	log.Fatal(http.ListenAndServe(":8080", nil))
-}
